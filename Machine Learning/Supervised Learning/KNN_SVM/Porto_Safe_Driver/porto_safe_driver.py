# -*- coding: utf-8 -*-
"""Porto_safe_driver.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IjOLUL9NGhPgqATS9IE4_7rpTnvYpUpC

Porto Seguro's Safe Driver Prediction
====

## Introduction
[This competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data) is hosted by the third largest insurance company in Brazil: Porto Seguro with the task of predicting the probability that a driver will initiate an insurance claim in the next year.

In the train data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). In addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features. Features without these designations are either continuous or ordinal. **Values of -1 indicate that the feature was missing** from the observation. The target columns signifies whether or not a claim was filed for that policy holder

train.csv contains the training data, where each row corresponds to a policy holder, and the target columns signifies that a claim was filed.
"""

# Import libraries
# Your code here
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Read csv file and take a look at it
train = pd.read_csv('./train.csv')
train.head(5)

# Taking a look at how many rows and columns the train dataset contains
rows = train.shape[0] 
columns = train.shape[1] 
print("The train dataset contains {} rows and {} columns".format(rows, columns))

"""## 1. Data Quality Checks

### Null or missing values check
Let check whether there are any null values in the train dataset as follows:
"""

train.isnull().values.any()

"""**Note:** Our null values check returns False but however, this does not really mean that this case has been closed as the data is also described as *"Values of -1 indicate that the feature was missing from the observation"*. Therefore I take it that Porto Seguro has simply conducted a blanket replacement of all null values in the data with the value of -1. Let us now inspect if there where any missing values in the data.

Here we can see that which columns contained -1 in their values so we could easily for example make a blanket replacement of all -1 with nulls first as follows:
"""

train_copy = train.copy()
train_copy = train_copy.replace(-1, np.NaN)

"""We can use "Missingno" to visualize missing values in the dataset."""

# If you don't have "Missingno" package, use this command to install
# (Linux) conda install -c conda-forge missingno
import missingno as msno

msno.matrix(df=train_copy.iloc[:, 2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))

# List null columns name
# Hint: isnull()
# Your code here
train.isnull().any()

"""#### Target variable inspection"""

# Using countplot to represent target
# Your code here
sns.countplot(train['target'])

"""## 2. Feature inspection and filtering

### Correlation plots
As a starter, let us generate some linear correlation plots just to have a quick look at how a feature is linearly correlated to the next and perhaps start gaining some insights from here. At this juncture, I will use the seaborn statistical visualisation package to plot a heatmap of the correlation values. Conveniently, Pandas dataframes come with the corr() method inbuilt, which calculates the Pearson correlation. Also as convenient is Seaborn's way of invoking a correlation plot. Just literally the word "heatmap"

#### Correlation of float features
"""

train_float = train.select_dtypes(include=['float64'])
train_int = train.select_dtypes(include=['int64'])

colormap = plt.cm.magma
plt.figure(figsize=(16, 12))
plt.title("Paerson correlation of continuous features", y=1.05, size=15)
sns.heatmap(train_float.corr(), linewidths=0.1, vmax=1.0, square=True,
           cmap=colormap)
plt.show()

"""From the correlation plot, we can see that the majority of the features display zero or no correlation to one another. This is quite an interesting observation that will warrant our further investigation later down. For now, the paired features that display a positive linear correlation are listed as follows:

(ps_reg_01, ps_reg_03)

(ps_reg_02, ps_reg_03)

(ps_car_12, ps_car_13)

(ps_car_13, ps_car_15)

### Correlation of integer features
"""

# Your code here
# Hint: correlation of train_int
colormap = plt.cm.magma
plt.figure(figsize=(16, 12))
plt.title("Paerson correlation of continuous features", y=1.05, size=15)
sns.heatmap(train_int.corr(), linewidths=0.1, vmax=1.0, square=True,
           cmap=colormap)
plt.show()

"""#### Which features have negatively correlation?
Negatively correlated features : (ps_calc_6, ps_calc_7)
                                 (ps_calc_17, ps_calc_18)

### Binary features inspection
Another aspect of the data that we may want to inspect would be the coulmns that only contain binary valeus, i.e where values take on only either of the two values 1 or 0. Proceeding, we store all columns that contain these binary values and then generate a vertical plotly bar plot of these binary values as follows:
"""

bin_col = list(filter(lambda x: '_bin' in x, train.columns))
bin_value_counts = train[bin_col].apply(pd.value_counts)

bin_value_counts

# hint pandas plot.bar with stacked=True
# Your code here
bin_value_counts.plot(kind='bar', stacked=True)

"""Those columns ps_ind_10_bin, ps_ind_11_bin, ps_ind_12_bin, ps_ind_13_bin which are completely dominated by zeros. They do not contain much information.

## 3. Preprocess Data
"""

# Import train_test_split 
# Your code here
from sklearn.model_selection import train_test_split

# Define X (features), y (target)
X = train.drop(columns=['id', 'target'])
y = train['target']

# Split data into train and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

print("Number of training dataset: ", len(X_train))
print("Number of testing dataset: ", len(X_test))
print("Total number: ", len(X_train)+len(X_test))

"""#### Randomly Under-Sampling the Training DatasetÂ¶
**Note:** Validate ratio between 2 class after Under-Sampling
"""

training_data = pd.concat ([X_train,y_train],axis = 1)
training_data['target'].value_counts()

percentage_safe = (train['target'] == 1).sum() / train.shape[0] * 100
percentage_no_safe = (train['target'] == 0).sum() / train.shape[0] * 100

print ('Percentage Safe: ', percentage_safe)
print ('Percentage Not Safe: ', percentage_no_safe)

print ('Percentage original safe: ', percentage_safe)
print ('Percentage original no-safe: ', percentage_no_safe)
number_of_instances = 100000

number_sub_safe = int (percentage_safe/100 * number_of_instances)
number_sub_no_safe = int (percentage_no_safe/100 * number_of_instances)

sub_safe_data = training_data[training_data['target'] == 1].head(number_sub_safe)
sub_no_safe_data = training_data[training_data['target'] == 0].head(number_sub_no_safe)

print ('Number of newly sub safe data:',len(sub_safe_data))
print ('Number of newly sub no-safe data:',len(sub_no_safe_data))

sub_training_data = pd.concat ([sub_safe_data, sub_no_safe_data], axis = 0)
sub_training_data['target'].value_counts()

safe_data = sub_safe_data[sub_safe_data['target'] == 1]

no_safe_data = sub_no_safe_data[sub_no_safe_data['target'] == 0]

# Number of fraud, non-fraud transactions
number_records_safe = safe_data.shape[0]
number_records_no_safe = no_safe_data.shape[0]

under_sample_no_safe = no_safe_data.sample(number_records_safe)
under_sample_safe = pd.concat([under_sample_no_safe, safe_data], axis=0)

print("Percentage of normal transactions: ", under_sample_no_safe.shape[0] / under_sample_safe.shape[0])
print("Percentage of fraud transactions: ", safe_data.shape[0] / under_sample_safe.shape[0])
print("Total number of transactions in resampled data: ", under_sample_safe.shape[0])


X_train_undersample = under_sample_safe.drop(columns=['target'])
y_train_undersample = under_sample_safe['target']

plt.figure(figsize=(7,7))

sns.countplot(data=under_sample_safe, x='target')
plt.show()

"""#### Randomly Over-Sampling the Training Dataset
**Note:** Validate ratio between 2 class after Over-Sampling
"""

safe_data = sub_safe_data[sub_safe_data['target'] == 1]
# Select row which "Class" is 0 and save in non_fraud_data
non_safe_data = sub_no_safe_data[sub_no_safe_data['target'] == 0]

# Number of fraud, non-fraud transactions
number_records_safe = safe_data.shape[0]
number_records_no_safe = no_safe_data.shape[0]

# Using sample on fraud_data with replacement "replace = True",  since we take a larger sample than population
over_sample_safe = safe_data.sample(replace = True, n=number_records_no_safe)
# **concat** over_sample_fraud and non_fraud_data to form under_sample_data
over_sample_no_safe = pd.concat([over_sample_safe, no_safe_data], axis=0)

# Showing ratio
print("Percentage of normal transactions: ", no_safe_data.shape[0]/over_sample_safe.shape[0])
print("Percentage of fraud transactions: ", safe_data.shape[0]/over_sample_safe.shape[0])
print("Total number of transactions in resampled data: ", over_sample_safe.shape[0])

# Assigning X, y for over-sampled dataset
X_train_oversample = over_sample_safe.drop(columns=['target'])
y_train_oversample = over_sample_safe['target']

# Plot countplot
plt.figure(figsize=(7,7))
# Make a count plot to show ratio between 2 class on "Class" column
sns.countplot(data=over_sample_no_safe, x='target')
plt.show()

"""## 4. Modeling"""

# Spot check with LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, BernoulliNB, GaussianNB
# Import libraries and and create model
# Your code here
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB


lr = LogisticRegression()
dtc = DecisionTreeClassifier()
rfc = RandomForestClassifier()
bnb = BernoulliNB()
gnb = GaussianNB()

models = [lr, dtc, rfc, gnb, bnb]
models_name = ["Logistic Regression", "Decision Tree", "Random Forest", "Bernoulli NB", "Gaussian NB"]

"""## 5. Evaluation Metrics"""

# Import confusion_matrix, classification_report
# Your code here
from sklearn.metrics import classification_report, confusion_matrix

# We create an utils function, that take a trained model as argument and print out confusion matrix
# classification report base on X and y
def evaluate_model(estimator, X, y):
    
    prediction = estimator.predict(X)
    print('Confusion matrix:', confusion_matrix(y, prediction))
    print('Classification report:\n', classification_report(y, prediction))
    np.set_printoptions(precision=2)
    model_name = type(estimator).__name__
    return {'name': model_name, 
            'recall': recall_score(y, prediction),
            'precision': precision_score(y, prediction),
           'description': description}

"""### Evaluate with Origin dataset"""

X_train_sub = sub_safe_data.drop(columns='target')
y_train_sub = sub_safe_data['target']
scores_origin = []

for idx, model in enumerate(models):
    print("Model: {}".format(models_name[idx]))
    model.fit(X_train_sub, y_train_sub)
    scores_origin.append(evaluate_model(model, X_test, y_test, 'origin'))
    print("=======================================")

"""### Evaluate with *Undersampled* dataset"""

scores_under = []
for idx, model in enumerate(models):
    print("Model: {}".format(models_name[idx]))
    model.fit(X_train_undersample, y_train_undersample)
    scores_under.append(evaluate_model(model, X_test, y_test, 'under'))
    print("=======================================")

"""### Evaluate with *Oversampled* dataset"""

scores_over = []
for idx, model in enumerate(models):
    print("Model: {}".format(models_name[idx]))
    model.fit(X_train_oversample, y_train_oversample)
    scores_over.append(evaluate_model(model, X_test, y_test, 'oversample'))
    
    # Evaluate model with X_test, y_test
    
    print("=======================================")

"""### Conclusion

Which model has lowest/highest accuracy?
Which model has lowest/highest recall?
Which model has lowest/highest f1?

## 6. GridsearchCV
"""

# Using gridsearchcv, random forest model and this param grid to find the best combination of parameters 
# Hint: example
# https://stackoverflow.com/questions/30102973/how-to-get-best-estimator-on-gridsearchcv-random-forest-classifier-scikit

param_grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
    'max_features': [2, 3],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 1000]
}